import requests
import json
from datetime import datetime, timezone
import time
from statistics import mean, median, stdev
import os # Necesario para verificar la existencia del archivo

# ... https://www.facebook.com/autoxec/videos/la-diferencia-entre-un-header-y-un-m%C3%BAltiple-original-radica-en-c%C3%B3mo-manejan-el-f/1599617577481601/ ...
URL = "https://api.wallapop.com/api/v3/search"
HEADERS = {
    "Host": "api.wallapop.com",
    "X-DeviceOS": "0"
}

# --------------------------------------------------------------------------
# NUEVA FUNCI√ìN: Cargar IDs del archivo existente
# --------------------------------------------------------------------------

def get_daily_filename():
    """Genera el nombre del archivo diario basado en la fecha UTC."""
    today = datetime.now(timezone.utc).strftime("%Y%m%d")
    return f"wallapop_motos_{today}.json"


def load_existing_ids(filename):
    """
    Lee el archivo JSON Lines y retorna un set con todos los IDs existentes.
    Esto permite la deduplicaci√≥n persistente entre ejecuciones del poller.
    """
    existing_ids = set()
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        # Asume que la l√≠nea es un objeto JSON v√°lido y extrae el 'id'
                        item = json.loads(line)
                        if item_id := item.get("id"):
                            existing_ids.add(item_id)
        except Exception as e:
            print(f"‚ö†Ô∏è Error al leer IDs existentes del archivo {filename}. Se comenzar√° sin IDs previos.")
            # Si el archivo est√° corrupto, es mejor empezar de cero que fallar.
            pass 
    return existing_ids

# --------------------------------------------------------------------------
# ... [El resto de funciones (fetch_all_pages, remove_duplicates, 
# is_clothing_or_personal_gear, filter_clothing_items, etc.) se mantienen igual] ...
# --------------------------------------------------------------------------

# Aqu√≠ ir√≠a el resto de tus funciones (remove_duplicates, filter_clothing_items, detect_suspicious_keywords, etc.)

def fetch_all_pages(keywords, category_id, latitude="41.648823", longitude="-0.889085"):
    """
    Obtiene TODOS los items de una b√∫squeda con paginaci√≥n,
    continuando hasta que la API no devuelva m√°s resultados para el d√≠a.
    """
    all_items = []
    offset = 0
    limit = 50
    page = 1
    
    # Bucle infinito: solo se detiene con 'break' si la API deja de devolver items.
    while True: 
        params = {
            "source": "search_box",
            "keywords": keywords,
            "category_id": str(category_id),
            "latitude": latitude,
            "longitude": longitude,
            "time_filter": "today",
            "order_by": "newest",
            "offset": offset,
            "limit": limit
        }
        
        try:
            response = requests.get(URL, params=params, headers=HEADERS, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            items = data.get("data", {}).get("section", {}).get("payload", {}).get("items", [])
            
            if not items:
                break
            
            all_items.extend(items)
            
            if len(items) < limit:
                break
            
            offset += limit
            page += 1
            time.sleep(0.5)
        
        except Exception as e:
            break
    
    return all_items

def remove_duplicates(items):
    """
    Elimina duplicados por ID, manteniendo el primero encontrado.
    Retorna lista limpia y estad√≠sticas.
    """
    seen_ids = set()
    unique_items = []
    duplicates_removed = 0
    
    for item in items:
        item_id = item.get("id")
        if item_id and item_id not in seen_ids:
            seen_ids.add(item_id)
            unique_items.append(item)
        else:
            duplicates_removed += 1
    
    return unique_items, duplicates_removed


# --------------------------------------------------------------------------
# FUNCI√ìN PARA EXCLUIR INDUMENTARIA DE MOTO
# --------------------------------------------------------------------------

def is_clothing_or_personal_gear(item):
    """Verifica si el √≠tem es probablemente indumentaria o accesorio personal
    de bajo valor, que se desea eliminar como ruido."""
    
    title = item.get("title", "").lower()
    description = item.get("description", "").lower()
    
    # Palabras clave de indumentaria y equipo personal que queremos EXCLUIR
    CLOTHING_KEYWORDS = [
        "casco", "guante", "chaqueta", "pantal√≥n", "pantalon", "botas", 
        "espaliers", "espalda", "goretex", "chamarra", "bota", "mono", 
        "traje", "talla", "alforja", "mochila","baul", "maleta", "chaleco",
        "protector", "protecci√≥n", "impermeable", "capa de lluvia ", "baul", "zapatos", "caballete", "mini",
        "herramientas", "herramientas", "candado", "antirrobo", "cubremanos", "alquiler","garaje"
    ] 
    
    # Aqu√≠ se mantiene la l√≥gica de filtrado de indumentaria para reducir el ruido
    if any(keyword in title for keyword in CLOTHING_KEYWORDS):
            return True 
    if any(keyword in description for keyword in CLOTHING_KEYWORDS):
            return True 

    return False

def filter_clothing_items(items):
    """Filtra y elimina √≠tems que parezcan ser indumentaria de moto."""
    filtered_items = []
    removed_count = 0
    for item in items:
        if not is_clothing_or_personal_gear(item):
            filtered_items.append(item)
        else:
            removed_count += 1
            
    return filtered_items, removed_count


def detect_suspicious_keywords(text):
    """Detecta palabras sospechosas en el texto."""

    SUSPICIOUS_KEYWORDS = {
        "sin_papeles": ["sin papeles", "sin documentacion", "sin documento", "no papeles", "papeles pendientes"],
        "sin_itv": ["sin itv", "sin itp", "no itv", "no itp", "itv caducada", "itp caducada"],
        "urgente": ["urgente", "solo hoy", "solo esta semana", "rapido", "r√°pido", "venta rapida"],
        "para_piezas": ["para piezas", "para despiece", "despiece", "solo piezas"],
        "robo_potencial": ["importacion", "importada", "procedencia dudosa", "comprada en", "encontrada"],
        "km_desconocido": ["km desconocidos", "kilometraje desconocido", "km incognita", "sin contar km"],
        "transferencia_pendiente": ["transferencia pendiente", "a mi nombre", "no esta a mi nombre", "nombre titular"],
        "precio_bajo": ["ganga", "precio bajo", "muy barato", "casi nuevo", "como nuevo", "oferta", "chollo", "solo hoy"],
    }
    
    if not text:
        return []
    
    text_lower = text.lower()
    found = []
    
    for category, keywords in SUSPICIOUS_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text_lower:
                found.append(keyword)
                break
    
    return found

def calculate_relative_price_index(price, all_prices):
    """Calcula el √≠ndice de precio relativo respecto a la media."""
    if not all_prices or not price:
        return 1.0
    
    avg_price = mean(all_prices)
    if avg_price == 0:
        return 1.0
    
    return round(price / avg_price, 2)

def calculate_risk_score(item, all_prices, seller_items_count):
    """Calcula risk_score de 0-100 basado en m√∫ltiples factores."""
    score = 0
    
    # Factor 1: Precio muy bajo (Secci√≥n 8.2 A.1)
    if all_prices:
        avg_price = mean(all_prices)
        price = item.get("price", {}).get("amount")
        if price:
            if price < avg_price * 0.4:
                score += 40
            elif price < avg_price * 0.6:
                score += 20
    
    # Factor 2: Palabras sospechosas (Secci√≥n 8.2 B)
    title = item.get("title", "")
    description = item.get("description", "")
    text = f"{title} {description}".lower()
    
    suspicious = detect_suspicious_keywords(text)
    score += min(len(suspicious) * 15, 30)
    
    # Factor 3: Descripci√≥n muy corta (Secci√≥n 8.2 D)
    if description and len(description) < 50:
        score += 10
    
    # Factor 4: Muchos anuncios del mismo vendedor (Secci√≥n 8.2 C.1)
    if seller_items_count and seller_items_count > 3:
        score += 20
    
    # Factor 5: Sin im√°genes (Se√±al de fraude)
    images = item.get("images", [])
    if not images:
        score += 5
    
    return min(score, 100)

def enrich_items(items):
    """Enriquece todos los items con campos calculados."""
    # Pre-calcular estad√≠sticas
    prices = [item.get("price", {}).get("amount") for item in items 
              if item.get("price", {}).get("amount")]
    prices = [p for p in prices if p is not None and p > 0]
    
    # Contar items por vendedor
    seller_counts = {}
    for item in items:
        seller_id = item.get("user_id")
        if seller_id:
            seller_counts[seller_id] = seller_counts.get(seller_id, 0) + 1
    
    enriched_items = []
    
    for item in items:
        enriched = item.copy()
        
        # Generar timestamp de rastreo
        enriched["crawl_timestamp"] = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
        
        price = item.get("price", {}).get("amount")
        enriched["relative_price_index"] = calculate_relative_price_index(price, prices)
        
        text = f"{item.get('title', '')} {item.get('description', '')}"
        suspicious_kw = detect_suspicious_keywords(text)
        
        seller_id = item.get("user_id")
        seller_count = seller_counts.get(seller_id, 0)
        
        # Creaci√≥n del diccionario de enriquecimiento (Secci√≥n 3.3)
        enriched["enrichment"] = {
            "suspicious_keywords": suspicious_kw,
            "suspicious_keywords_count": len(suspicious_kw),
            "risk_score": calculate_risk_score(item, prices, seller_count),
            "relative_price_index": enriched["relative_price_index"],
            "seller_items_today": seller_count,
            "description_length": len(item.get("description", "")),
            "has_images": len(item.get("images", [])) > 0,
            "image_count": len(item.get("images", []))
        }
        
        enriched_items.append(enriched)
    
    return enriched_items

def save_daily_file(all_items, filename):
    """Guarda todos los items en fichero diario (JSON Lines format)"""
    # *** MODO "a" (append/anexar) para acumular registros ***
    with open(filename, "a", encoding="utf-8") as f:
        for item in all_items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    

def print_statistics(items):
    """Imprime estad√≠sticas del dataset"""
    print("\n" + "=" * 70)
    print("üìä ESTAD√çSTICAS DEL DATASET")
    print("=" * 70)
    
    prices = [item.get("price", {}).get("amount") for item in items 
              if item.get("price", {}).get("amount")]
    prices = [p for p in prices if p and p > 0]
    
    risk_scores = [item.get("enrichment", {}).get("risk_score", 0) for item in items]
    
    if prices:
        print(f"üí∞ Precios:")
        print(f" ¬† Min: {min(prices):.2f}‚Ç¨ | Max: {max(prices):.2f}‚Ç¨ | Media: {mean(prices):.2f}‚Ç¨")
        if len(prices) > 1:
            print(f" ¬† Mediana: {median(prices):.2f}‚Ç¨ | Desv. Std: {stdev(prices):.2f}‚Ç¨")
    
    if risk_scores:
        print(f"\n‚ö†Ô∏è ¬†Risk Scores:")
        print(f" ¬† Min: {min(risk_scores)} | Max: {max(risk_scores)} | Media: {mean(risk_scores):.1f}")
        
        # Umbrales de riesgo
        high_risk = len([r for r in risk_scores if r >= 70])
        medium_risk = len([r for r in risk_scores if 40 <= r < 70])
        low_risk = len([r for r in risk_scores if r < 40])
        
        print(f"\n ¬† üî¥ ALTO RIESGO (>=70): {high_risk}")
        print(f" ¬† üü° RIESGO MEDIO (40-69): {medium_risk}")
        print(f" ¬† üü¢ BAJO RIESGO (<40): {low_risk}")
    
    print("=" * 70 + "\n")

if __name__ == "__main__":
    
    print("=" * 70)
    print("Obteniendo MOTOS, PIEZAS Y ACCESORIOS (EXCLUYENDO INDUMENTARIA) de hoy")
    print("=" * 70 + "\n")
    
    # --- Deduplicaci√≥n Persistente: Cargar IDs existentes ---
    daily_filename = get_daily_filename()
    existing_ids = load_existing_ids(daily_filename)
    print(f"Cargados {len(existing_ids)} IDs existentes para el d√≠a.")
    
    all_items = []
    
    # 1. B√∫squeda
    search_queries = [
        ("moto", 14000),
    ]
    
    for keywords, category_id in search_queries:
        print(f"üîç Buscando: '{keywords}' (category_id={category_id})")
        items = fetch_all_pages(keywords, category_id) 
        
        if items:
            print(f" ¬† ‚Üí Total para '{keywords}' adquirido: {len(items)} items\n")
            all_items.extend(items)
        else:
            print(f" ¬† ‚Ñπ No se encontraron items\n")
    
    # 2. Eliminaci√≥n de duplicados (internos de la adquisici√≥n)
    print(f"Eliminando duplicados internos de {len(all_items)} items...")
    unique_items, dupes_internal = remove_duplicates(all_items)
    print(f"‚úì Items √∫nicos (internos): {len(unique_items)} | Duplicados eliminados: {dupes_internal}\n")
    
    # 3. FILTRADO DE INDUMENTARIA
    print("Filtro de ruido: eliminando indumentaria y equipo personal...")
    filtered_items, removed_clothing_count = filter_clothing_items(unique_items)
    print(f"‚úì √çtems descartados (Indumentaria): {removed_clothing_count}")
    print(f"‚úì √çtems para an√°lisis: {len(filtered_items)}\n")
    
    # 4. FILTRADO DE DUPLICADOS PERSISTENTES (¬°CLAVE!)
    new_items_to_save = [
        item for item in filtered_items if item.get("id") not in existing_ids
    ]
    duplicates_external = len(filtered_items) - len(new_items_to_save)
    print(f"‚úì √çtems descartados (ya estaban en JSON): {duplicates_external}")
    print(f"‚úì √çtems NUEVOS listos para guardar: {len(new_items_to_save)}\n")
    
    # 5. Enriquecimiento (SOLO los nuevos)
    print("Enriqueciendo SOLO datos nuevos...")
    enriched_new_items = enrich_items(new_items_to_save)
    print(f"‚úì {len(enriched_new_items)} items enriquecidos\n")
    
    # 6. Estad√≠sticas (basadas en todos los √≠tems √∫nicos del d√≠a, no solo los nuevos)
    # Para estad√≠sticas completas del d√≠a, deber√≠as cargar todos los items existentes
    # y los nuevos, pero para mantenerlo simple, imprimimos solo sobre la nueva tanda.
    if enriched_new_items:
        print_statistics(enriched_new_items)
    
    # 7. Guardar
    if enriched_new_items:
        save_daily_file(enriched_new_items, daily_filename)
        print(f"‚úÖ Archivo guardado: {daily_filename}. Los datos se anexar√°n en futuras llamadas.")
    else:
        print("\n‚ö†Ô∏è ¬†No hay datos NUEVOS para guardar.")