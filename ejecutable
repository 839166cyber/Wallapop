import requests
import json
from datetime import datetime, timezone
import time
from statistics import mean, median, stdev




URL = "https://api.wallapop.com/api/v3/search"
HEADERS = {
    "Host": "api.wallapop.com",
    "X-DeviceOS": "0"
}


def fetch_all_pages(keywords, category_id, latitude="41.648823", longitude="-0.889085", max_pages=10):
    """Obtiene TODOS los items de una bÃºsqueda con paginaciÃ³n."""
    all_items = []
    offset = 0
    limit = 100
    page = 1
    
    while page <= max_pages:
        params = {
            "source": "search_box",
            "keywords": keywords,
            "category_id": str(category_id),
            "latitude": latitude,
            "longitude": longitude,
            "time_filter": "today",
            "order_by": "newest",
            "offset": offset,
            "limit": limit
        }
        
        try:
            print(f"   PÃ¡gina {page} (offset={offset})...", end=" ", flush=True)
            
            response = requests.get(URL, params=params, headers=HEADERS, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            items = data.get("data", {}).get("section", {}).get("payload", {}).get("items", [])
            
            if not items:
                print("(sin mÃ¡s items)")
                break
            
            print(f"âœ“ {len(items)} items")
            all_items.extend(items)
            
            if len(items) < limit:
                print(f"   â†’ Ãšltima pÃ¡gina alcanzada")
                break
            
            offset += limit
            page += 1
            time.sleep(0.5)
        
        except Exception as e:
            print(f"âŒ Error: {e}")
            break
    
    return all_items

def remove_duplicates(items):
    """
    Elimina duplicados por ID, manteniendo el primero encontrado.
    Retorna lista limpia y estadÃ­sticas.
    """
    seen_ids = set()
    unique_items = []
    duplicates_removed = 0
    
    for item in items:
        item_id = item.get("id")
        if item_id and item_id not in seen_ids:
            seen_ids.add(item_id)
            unique_items.append(item)
        else:
            duplicates_removed += 1
    
    return unique_items, duplicates_removed

def detect_suspicious_keywords(text):
    """Detecta palabras sospechosas en el texto."""
    SUSPICIOUS_KEYWORDS = {
        "sin_papeles": ["sin papeles", "sin documentacion", "sin documento", "no papeles", "papeles pendientes"],
        "sin_itv": ["sin itv", "sin itp", "no itv", "no itp", "itv caducada", "itp caducada"],
        "urgente": ["urgente", "solo hoy", "solo esta semana", "rapido", "rÃ¡pido", "venta rapida"],
        "para_piezas": ["para piezas", "para despiece", "despiece", "solo piezas"],
        "robo_potencial": ["importacion", "importada", "procedencia dudosa", "comprada en", "encontrada"],
        "km_desconocido": ["km desconocidos", "kilometraje desconocido", "km incognita", "sin contar km"],
        "transferencia_pendiente": ["transferencia pendiente", "a mi nombre", "no esta a mi nombre", "nombre titular"],
        "precio_bajo": ["ganga", "precio bajo", "muy barato", "casi nuevo", "como nuevo"],
    }
    
    if not text:
        return []
    
    text_lower = text.lower()
    found = []
    
    for category, keywords in SUSPICIOUS_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text_lower:
                found.append(keyword)
                break
    
    return found

def calculate_relative_price_index(price, all_prices):
    """Calcula el Ã­ndice de precio relativo respecto a la media."""
    if not all_prices or not price:
        return 1.0
    
    avg_price = mean(all_prices)
    if avg_price == 0:
        return 1.0
    
    return round(price / avg_price, 2)

def calculate_risk_score(item, all_prices, seller_items_count):
    """Calcula risk_score de 0-100 basado en mÃºltiples factores."""
    score = 0
    
    # Factor 1: Precio muy bajo
    if all_prices:
        avg_price = mean(all_prices)
        if price := item.get("price", {}).get("amount"):
            if price < avg_price * 0.4:
                score += 40
            elif price < avg_price * 0.6:
                score += 20
    
    # Factor 2: Palabras sospechosas
    title = item.get("title", "")
    description = item.get("description", "")
    text = f"{title} {description}".lower()
    
    suspicious = detect_suspicious_keywords(text)
    score += min(len(suspicious) * 15, 30)
    
    # Factor 3: DescripciÃ³n muy corta
    if description and len(description) < 50:
        score += 10
    
    # Factor 4: Muchos anuncios del mismo vendedor
    if seller_items_count and seller_items_count > 3:
        score += 20
    
    # Factor 5: Sin imÃ¡genes
    images = item.get("images", [])
    if not images:
        score += 5
    
    return min(score, 100)

def enrich_items(items):
    """Enriquece todos los items con campos calculados."""
    # Pre-calcular estadÃ­sticas
    prices = [item.get("price", {}).get("amount") for item in items 
              if item.get("price", {}).get("amount")]
    prices = [p for p in prices if p is not None and p > 0]
    
    # Contar items por vendedor
    seller_counts = {}
    for item in items:
        seller_id = item.get("user_id")
        if seller_id:
            seller_counts[seller_id] = seller_counts.get(seller_id, 0) + 1
    
    enriched_items = []
    
    for item in items:
        enriched = item.copy()
        
        enriched["crawl_timestamp"] = datetime.now(timezone.utc).isoformat() + "Z"
        
        price = item.get("price", {}).get("amount")
        enriched["relative_price_index"] = calculate_relative_price_index(price, prices)
        
        text = f"{item.get('title', '')} {item.get('description', '')}"
        suspicious_kw = detect_suspicious_keywords(text)
        
        seller_id = item.get("user_id")
        seller_count = seller_counts.get(seller_id, 0)
        
        enriched["enrichment"] = {
            "suspicious_keywords": suspicious_kw,
            "suspicious_keywords_count": len(suspicious_kw),
            "risk_score": calculate_risk_score(item, prices, seller_count),
            "relative_price_index": enriched["relative_price_index"],
            "seller_items_today": seller_count,
            "description_length": len(item.get("description", "")),
            "has_images": len(item.get("images", [])) > 0,
            "image_count": len(item.get("images", []))
        }
        
        enriched_items.append(enriched)
    
    return enriched_items

def save_daily_file(all_items, filename):
    """Guarda todos los items en fichero diario (JSON Lines format)"""
    with open(filename, "w", encoding="utf-8") as f:
        for item in all_items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"âœ“ Guardados {len(all_items)} anuncios en {filename}\n")

def print_statistics(items):
    """Imprime estadÃ­sticas del dataset"""
    print("\n" + "=" * 70)
    print("ðŸ“Š ESTADÃSTICAS DEL DATASET")
    print("=" * 70)
    
    prices = [item.get("price", {}).get("amount") for item in items 
              if item.get("price", {}).get("amount")]
    prices = [p for p in prices if p and p > 0]
    
    risk_scores = [item.get("enrichment", {}).get("risk_score", 0) for item in items]
    
    if prices:
        print(f"ðŸ’° Precios:")
        print(f"   Min: {min(prices):.2f}â‚¬ | Max: {max(prices):.2f}â‚¬ | Media: {mean(prices):.2f}â‚¬")
        if len(prices) > 1:
            print(f"   Mediana: {median(prices):.2f}â‚¬ | Desv. Std: {stdev(prices):.2f}â‚¬")
    
    if risk_scores:
        print(f"\nâš ï¸  Risk Scores:")
        print(f"   Min: {min(risk_scores)} | Max: {max(risk_scores)} | Media: {mean(risk_scores):.1f}")
        
        high_risk = len([r for r in risk_scores if r >= 70])
        medium_risk = len([r for r in risk_scores if 40 <= r < 70])
        low_risk = len([r for r in risk_scores if r < 40])
        
        print(f"\n   ðŸ”´ ALTO RIESGO (>=70): {high_risk}")
        print(f"   ðŸŸ¡ RIESGO MEDIO (40-69): {medium_risk}")
        print(f"   ðŸŸ¢ BAJO RIESGO (<40): {low_risk}")
    
    print("=" * 70 + "\n")

if __name__ == "__main__":
    print("=" * 70)
    print("Obteniendo MOTOS Y CICLOMOTORES de hoy (CON ENRIQUECIMIENTO)")
    print("=" * 70 + "\n")
    
    all_items = []
    
    search_queries = [
        ("moto", 12800),
        ("ciclomotor", 12800),
        ("scooter", 12800),
    ]
    
    for keywords, category_id in search_queries:
        print(f"ðŸ” Buscando: '{keywords}' (category_id={category_id})")
        items = fetch_all_pages(keywords, category_id, max_pages=10)
        
        if items:
            print(f"   â†’ Total para '{keywords}': {len(items)} items\n")
            all_items.extend(items)
        else:
            print(f"   â„¹ No se encontraron items\n")
    
    # Eliminar duplicados (MEJORADO)
    print(f"Eliminando duplicados de {len(all_items)} items...")
    unique_items, dupes = remove_duplicates(all_items)
    print(f"âœ“ Items Ãºnicos: {len(unique_items)} | Duplicados eliminados: {dupes}\n")
    
    # ENRIQUECER
    print("Enriqueciendo datos...")
    enriched_items = enrich_items(unique_items)
    print(f"âœ“ {len(enriched_items)} items enriquecidos\n")
    
    # EstadÃ­sticas
    print_statistics(enriched_items)
    
    # Guardar
    if enriched_items:
        today = datetime.now(timezone.utc).strftime("%Y%m%d")
        filename = f"wallapop_motos_ciclomotores_{today}.json"
        save_daily_file(enriched_items, filename)
        print(f"âœ… Archivo guardado: {filename}")
    else:
        print("\nâš ï¸  No hay datos para guardar")
